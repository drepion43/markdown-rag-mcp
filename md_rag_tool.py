from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from mcp.server.fastmcp import FastMCP
from dotenv import load_dotenv
from typing import List, Dict, Any, Optional
from langchain.prompts import PromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema import Document
from langchain.schema.runnable import Runnable


# API í‚¤ ì •ë³´ ë¡œë“œ
load_dotenv(override=True)

class RAGChain:
    def __init__(self,
                 llm: object,
                 embeddings: Optional[object] = None):
        self.llm = llm
        self.embeddings = embeddings or OpenAIEmbeddings(model="text-embedding-ada-002")

    def create_retriever(self, markdown_text: str):
        """Markdown í…ìŠ¤íŠ¸ â†’ Retriever ìƒì„±"""
        doc = Document(page_content=markdown_text)
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        split_docs = splitter.split_documents([doc])
        vector = FAISS.from_documents(split_docs, self.embeddings)
        return vector.as_retriever()

    def format_docs(self, docs: List[Document]) -> str:
        """ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³‘í•©"""
        return "\n".join([doc.page_content for doc in docs])

    async def invoke(self, inputs: Dict[str, Any]) -> str:
        question: str = inputs.get("question", "")
        context_docs: List[Document] = inputs.get("context", [])
        history: List[str] = inputs.get("chat_history", [])

        context_text = self.format_docs(context_docs) if isinstance(context_docs, list) else context_docs
        history_text = "\n".join(history)

        prompt = PromptTemplate.from_template(
                """
                ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AIì…ë‹ˆë‹¤.

                ğŸ”¹ ê³¼ê±° ëŒ€í™” ë‚´ìš© (ì°¸ê³ ìš©):
                {chat_history}
                (ê³¼ê±° ëŒ€í™”ëŠ” ì‚¬ìš©ìê°€ ì´ì „ì— ë¬¼ì–´ë³¸ ì§ˆë¬¸ê³¼ ê·¸ì— ëŒ€í•œ ë‹µë³€ë“¤ì…ë‹ˆë‹¤. ì´ ëŒ€í™”ëŠ” ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì´í•´í•˜ëŠ” ë° ì¤‘ìš”í•˜ë©°, ì‚¬ìš©ìê°€ ê³„ì†í•´ì„œ ì§„í–‰í•˜ê³ ì í•˜ëŠ” ëŒ€í™”ì˜ íë¦„ì„ ì¶”ì í•´ì•¼ í•©ë‹ˆë‹¤.)

                ğŸ”¹ ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© (ë°˜ë“œì‹œ ì°¸ê³ ):
                {context}
                (ê²€ìƒ‰ëœ ë¬¸ì„œëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì œê³µí•  ë•Œ ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ì…ë‹ˆë‹¤. ë¬¸ì„œì—ì„œ ì œê³µëœ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.)

                ğŸ’¬ ì‚¬ìš©ì ì§ˆë¬¸:
                {question}
                (ì´ ì§ˆë¬¸ì€ ì‚¬ìš©ìê°€ í˜„ì¬ ì›í•˜ëŠ” ì •ë³´ì…ë‹ˆë‹¤. ë‹µë³€ì€ ì´ ì§ˆë¬¸ì„ ì§ì ‘ì ìœ¼ë¡œ ë‹¤ë¤„ì•¼ í•©ë‹ˆë‹¤.)

                ğŸ“Œ ì§€ì¹¨:
                - ê³¼ê±° ëŒ€í™” ë‚´ìš©ì€ í˜„ì¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë§¥ë½ì„ ì œê³µí•©ë‹ˆë‹¤. ëŒ€í™”ì˜ íë¦„ì„ ë†“ì¹˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì„¸ìš”.
                - ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì—ë§Œ ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
                - ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ì„œì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.
                - ê°„ê²°í•˜ê³  ì „ë¬¸ì ì¸ ë¬¸ì¥ìœ¼ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”.

                ğŸ§  ë‹µë³€:
                """
            )

        chain: Runnable = prompt | self.llm | StrOutputParser()

        try:
            response = await chain.ainvoke({
                "chat_history": history_text,
                "context": context_text,
                "question": question
            })
        except Exception as e:
            response = f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

        return response
    
# Initialize FastMCP server with configuration
mcp = FastMCP(
    "Retriever",
    instructions="A Retriever that can retrieve information from the database.",
    host="0.0.0.0",
    port=8005,
)

# RAG ì‹¤í–‰ í•¨ìˆ˜
@mcp.tool()
async def run_rag(question: str,
                  context: str,
                  history: Optional[str]):
    # LLMê³¼ Embeddings ì„¤ì •
    embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.5)

    # RAGChain ìƒì„±
    rag_chain = RAGChain(llm=llm, embeddings=embeddings)

    # ë¬¸ì„œ ê²€ìƒ‰ê¸° ìƒì„±
    retriever = rag_chain.create_retriever(context)

    # RAG ì‹¤í–‰
    response = rag_chain.invoke({
        "question": question,
        "context": retriever.get_relevant_documents(question),
        "chat_history": history
    })
    
    return response

if __name__ == "__main__":
    # Run the MCP server with stdio transport for integration with MCP clients
    mcp.run(transport="stdio")